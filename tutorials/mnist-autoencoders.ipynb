{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hands-on session 2.1: Variational Autoencoder\n",
    "## Building Autoencoders in PyTorch\n",
    "\n",
    "Made by **Nathan Painchaud** and **Pierre-Marc Jodoin** from the Université de Sherbrooke, Canada.\n",
    "\n",
    "Inspired by a [similar tutorial](https://blog.keras.io/building-autoencoders-in-keras.html) for Keras by François Chollet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if '../' in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append('../')\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's start by looking at our data\n",
    "In the next cell, we will load the MNIST dataset and visualize some of its images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from src.visualization.utils import display_data_samples\n",
    "\n",
    "# MNIST consists of 28x28 images, so the size of the data is\n",
    "data_shape = 28, 28\n",
    "data_size = data_shape[0] * data_shape[1]\n",
    "\n",
    "# Download and prepare data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mnist_train = MNIST(\"../../../input/dlss21_autoencoder\", train=True, transform=transform)\n",
    "mnist_test = MNIST(\"../../../input/dlss21_autoencoder\", train=False, transform=transform)\n",
    "\n",
    "# Check data by displaying random images\n",
    "samples_indices = np.random.randint(len(mnist_train), size=10)\n",
    "mnist_img_list = [mnist_train[sample_idx][0] for sample_idx in samples_indices]\n",
    "display_data_samples(data=mnist_img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's build a deep deterministic autoencoder\n",
    "\n",
    "Here, we will build a simple auto-encoder with only **dense** (aka fully-connected) layers and **ReLUs**.  In pytorch, a dense layer is dubbed **Linear**.\n",
    "\n",
    "Both the encoder and the decocer have **3 layers** and the latent space has **32 dimensions**.\n",
    "\n",
    "Since the pixels have values between 0 and 1, the last activation function is a **Sigmoid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Let's define the encoder architecture we want,\n",
    "# with some options to configure the input and output size\n",
    "def make_encoder(data_size, latent_space_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(data_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, latent_space_size),\n",
    "    )\n",
    "\n",
    "# Same thing for the decoder\n",
    "def make_decoder(data_size, latent_space_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(latent_space_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, data_size),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "# Now let's build our networks, with an arbitrary dimensionality of the latent space\n",
    "# and an input and output size depending on the data.\n",
    "encoder = make_encoder(data_size, 32)\n",
    "decoder = make_decoder(data_size, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "    \n",
    "* Can you see how the architecture of the encoder is the dual of that of the decoder?\n",
    "* Why do you think that the decoder has an output **sigmoid** activation function?\n",
    "* What is the latent space size of the autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm\n",
    "\n",
    "Before we can train our model, we have to define our training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define some training hyperparameters\n",
    "epochs = 25\n",
    "batch_size = 256\n",
    "\n",
    "def train(forward_pass_fn, encoder, decoder, optimizer, train_data, val_data, device=\"cuda\"):\n",
    "    # Create dataloaders from the data\n",
    "    # Those are PyTorch's abstraction to help iterate over the data\n",
    "    data_loader_kwargs = {\"batch_size\": batch_size, \"num_workers\": os.cpu_count() - 1, \"pin_memory\": True}\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, **data_loader_kwargs)\n",
    "    val_dataloader = DataLoader(val_data, **data_loader_kwargs)\n",
    "\n",
    "    # Ensure that the networks are on the requested device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    fit_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    pbar_metrics = {\"train_loss\": None, \"val_loss\": None}\n",
    "    for epoch in fit_pbar:\n",
    "        # Set model in training mode before training\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Train once over all the training data\n",
    "        for x, _ in train_dataloader:\n",
    "            x = x.to(device)    # Move the data tensor to the device\n",
    "            optimizer.zero_grad()   # Make sure gradients are reset\n",
    "            train_loss, _ = forward_pass_fn(encoder, decoder, x)    # Forward pass\n",
    "            train_loss.backward()   # Backward pass\n",
    "            optimizer.step()    # Update parameters w.r.t. optimizer and gradients\n",
    "            pbar_metrics[\"train_loss\"] = train_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        # Set model in eval mode before validation\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # At the end of the epoch, check performance against the validation data\n",
    "        for x, _ in val_dataloader:\n",
    "            x = x.to(device)    # Move the data tensor to the device\n",
    "            val_loss, _ = forward_pass_fn(encoder, decoder, x)\n",
    "            pbar_metrics[\"val_loss\"] = val_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also have to define a our **forward pass** algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def autoencoder_forward_pass(encoder, decoder, x):\n",
    "    \"\"\"AE forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that predicts a latent vector\n",
    "        decoder: neural net that projects a point in the latent space back into the image space\n",
    "        x: batch of N MNIST images\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy loss\n",
    "        x_hat: batch of N reconstructed images\n",
    "    \"\"\"\n",
    "    in_shape = x.shape  # Save the input shape\n",
    "    encoder_input = torch.flatten(x, start_dim=1)   # Flatten the 2D image to a 1D tensor (for the linear layer)\n",
    "    z = encoder(encoder_input)  # Forward pass on the encoder (to get the latent space vector)\n",
    "    x_hat = decoder(z)  # Forward pass on the decoder (to get the reconstructed input)\n",
    "    x_hat = x_hat.reshape(in_shape)    # Restore the output to the original shape\n",
    "    loss = F.binary_cross_entropy(x_hat, x) # Compute the reconstruction loss\n",
    "    return loss, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "The previous *train(.)* function contains a typical **pytorch training loop**.\n",
    "\n",
    "* Do you see what the data loaders are used for?\n",
    "* Do you see what the forward pass does?  What are the inputs and outputs of that function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([*encoder.parameters(), *decoder.parameters()])\n",
    "train(autoencoder_forward_pass, encoder, decoder, optimizer, mnist_train, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's take a look at the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.visualization.utils import display_autoencoder_results\n",
    "\n",
    "display_autoencoder_results(mnist_test, lambda x: autoencoder_forward_pass(encoder, decoder, x.cuda())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's turn our autoencoder variational\n",
    "\n",
    "Variational autoencoders (VAE) are very similar to autoencoders.  The differences are threefold:\n",
    "\n",
    "* The VAE's encoder ouputs mean and variance vectors\n",
    "* The input of the decoder is a vector, randomly sampled, from a Normal distribution determined by the predicted mean and variance vectors \n",
    "* The loss has 2 terms: the reconstruction loss (like for the normal AE) + the KL divergence (for the encoder's output)\n",
    "\n",
    "Since gradient cannot back-propagate into a random sampling method, VAE always come with a **reparametrization trick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# This time, we make the latent space 2-dimensional to visualize it easily afterwards\n",
    "latent_space_size = 2\n",
    "\n",
    "# In practice, a small trick to easily implement the two heads of the encoder is to simply\n",
    "# double the size of its output. Then, we can slice the output in half during the forward pass!\n",
    "vae_encoder = make_encoder(data_size, latent_space_size * 2) \n",
    "vae_decoder = make_decoder(data_size, latent_space_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "* In the previous cell, we used the same function to build our VAE's encoder and decoder networks than the AE.  The only difference is the output size of the encoder is multiplied by 2.  Why do you think that is?\n",
    "* In the next cell, we include the **reparametrization trick** to the **forward pass**.  Remember why this has to be done?\n",
    "* What is the latent space size of the VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to change the training algorithm, since we have to implement the **reparametrization trick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code"
    }
   },
   "outputs": [],
   "source": [
    "def kl_div(mu, logvar):\n",
    "    kl_div_by_samples = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kl_div_by_samples)\n",
    "\n",
    "def vae_forward_pass(encoder, decoder, x):\n",
    "    \"\"\"VAE forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that predicts a mean and a logvar vector\n",
    "        decoder: neural net that projects a point in the latent space back into the image space\n",
    "        x: batch of N MNIST images\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy + kl_divergence loss\n",
    "        x_hat: batch of N reconstructed images\n",
    "    \"\"\"\n",
    "    in_shape = x.shape  # Save the input shape\n",
    "    encoder_input = torch.flatten(x, start_dim=1)  # Flatten the 2D image to a 1D tensor (for the linear layer)\n",
    "    encoding_distr = encoder(encoder_input)  # Forward pass on the encoder (to get the latent space posterior)\n",
    "    # Nothing changed so far!\n",
    "\n",
    "    # Second part of our trick!\n",
    "    # We separate the (unique) latent space posterior into its two halves: mean and logvar\n",
    "    mu, logvar = encoding_distr[:, :latent_space_size], encoding_distr[:, latent_space_size:]\n",
    "\n",
    "    # Reparametrization trick\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    z = mu + eps * std\n",
    "\n",
    "    # Decoding mostly stays the same. The only difference is the added 4th line below\n",
    "    x_hat = decoder(z)  # Forward pass on the decoder (to get the reconstructed input)\n",
    "    x_hat = x_hat.reshape(in_shape)    # Restore the output to the original shape\n",
    "    loss = F.binary_cross_entropy(x_hat, x) # Compute the reconstruction loss\n",
    "    loss += 1e-5 * kl_div(mu, logvar)  # Loss now also includes the KL divergence term\n",
    "    return loss, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now it's time to train our variational autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([*vae_encoder.parameters(), *vae_decoder.parameters()])\n",
    "train(vae_forward_pass, vae_encoder, vae_decoder, optimizer, mnist_train, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "display_autoencoder_results(mnist_test, lambda x: vae_forward_pass(vae_encoder, vae_decoder, x.cuda())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question :\n",
    "\n",
    "Why do you think that with only 2 dimensions in the latent space we get to reconstruct good-looking images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## More visualization \n",
    "\n",
    "Now that we have a latent space in two dimensions, we can easily visualize it and look at how the data is\n",
    "distributed.\n",
    "\n",
    "> NOTE: Because of technical problems with the FloydHub platform, the interactive latent space visualization of the\n",
    "> latent space does not work as expected, and instead we can only produce a static reconstruction of the sample at the\n",
    "> (0,0) coordinate in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.visualization.latent_space import explore_latent_space\n",
    "\n",
    "explore_latent_space(\n",
    "    mnist_test,\n",
    "    lambda x: vae_encoder(torch.flatten(x, start_dim=1))[:, :latent_space_size],\n",
    "    lambda z: vae_decoder(z).reshape(data_shape),\n",
    "    encodings_label=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we shall decode one selected vector `z` in the latent space.  Change the content of that vector and you will see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = [6, 0]  # 2D latent vector\n",
    "\n",
    "z_torch = torch.Tensor(np.array(z)).cuda()  # convert Z into a PyTorch tensor\n",
    "\n",
    "sample = vae_decoder(z_torch).reshape(data_shape)  # decode the latent vector with the VAE decoder\n",
    "\n",
    "plt.imshow(sample.detach().cpu().numpy()) # plot the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
