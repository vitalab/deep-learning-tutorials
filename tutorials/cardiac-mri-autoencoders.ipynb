{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on session 2.1: Variational Autoencoder\n",
    "## Variational Autoencoders Applied to Cardiac MRI\n",
    "\n",
    "Made by **Nathan Painchaud** and **Pierre-Marc Jodoin** from the Universit√© de Sherbrooke, Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if '../' in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append('../')\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Once again, let's start by looking at our data\n",
    "Here we load MRI cardiac images and their groundtruth segmentation maps from the **ACDC dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.data.acdc.dataset import Acdc\n",
    "from src.visualization.utils import display_data_samples\n",
    "\n",
    "# ACDC consists of 256x256 images with segmentation maps for 3 classes + background, so the size of the data is\n",
    "data_shape = (4, 256, 256)\n",
    "\n",
    "# Download and prepare data\n",
    "acdc_train = Acdc(\"../data/acdc.h5\", image_set=\"train\")\n",
    "acdc_val = Acdc(\"../data/acdc.h5\", image_set=\"val\")\n",
    "\n",
    "# Check data by displaying random images\n",
    "samples_indices = np.random.randint(len(acdc_train), size=10)\n",
    "imgs, gts = zip(*[acdc_train[sample_idx] for sample_idx in samples_indices])\n",
    "display_data_samples(mri=imgs, segmentation=gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a deep autoencoder specialized for image processing: a convolutional autoencoder\n",
    "\n",
    "Since convolutional networks are more complex than fully-connected networks and require a bit more code, let's tackle\n",
    "one half of the autoencoder at a time. Let's start with the **encoder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Let's define the encoder architecture we want,\n",
    "# with some options to configure the input and output size\n",
    "def downsampling_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def make_encoder(data_shape, latent_space_size):\n",
    "    in_channels = data_shape[0]\n",
    "    shape_at_bottleneck = data_shape[1] // 16, data_shape[2] // 16\n",
    "    size_at_bottleneck = shape_at_bottleneck[0] * shape_at_bottleneck[1] * 48\n",
    "    return nn.Sequential(\n",
    "        downsampling_block(in_channels, 48),    # Block 1 (input)\n",
    "        downsampling_block(48, 96),             # Block 2\n",
    "        downsampling_block(96, 192),            # Block 3\n",
    "        downsampling_block(192, 48),            # Block 4 (limits number of channels to reduce total number of parameters)\n",
    "        nn.Flatten(),                           # Flatten before FC-layer at the bottleneck\n",
    "        nn.Linear(size_at_bottleneck, latent_space_size),   # Bottleneck\n",
    "    )\n",
    "\n",
    "# Now let's build our encoder, with an arbitrary dimensionality of the latent space\n",
    "# and an input size depending on the data.\n",
    "latent_space_size = 32\n",
    "encoder = make_encoder(data_shape, latent_space_size*2) # here the latent space size is *2 because the encoder predicts a *mean* and *variance* vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the structure of the encoder that we have just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary_kwargs = dict(col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0)\n",
    "\n",
    "summary(encoder, input_size=data_shape, batch_dim=0,  **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the encoder is good, let's make a decoded that mirrors the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.modules import layers\n",
    "\n",
    "# Same building blocks for the decoder as for the encoder\n",
    "def upsampling_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def make_decoder(data_shape, latent_space_size):\n",
    "    out_channels = data_shape[0]\n",
    "    shape_at_bottleneck = data_shape[1] // 16, data_shape[2] // 16\n",
    "    size_at_bottleneck = shape_at_bottleneck[0] * shape_at_bottleneck[1] * 48\n",
    "    return nn.Sequential(\n",
    "        # Bottleneck\n",
    "        nn.Linear(latent_space_size, size_at_bottleneck),\n",
    "        nn.ReLU(),\n",
    "        layers.Reshape((48, *shape_at_bottleneck)),    # Restore shape before convolutional layers\n",
    "\n",
    "        upsampling_block(48, 192),     # Block 1\n",
    "        upsampling_block(192, 96),     # Block 2\n",
    "        upsampling_block(96, 48),      # Block 3\n",
    "        nn.ConvTranspose2d(in_channels=48, out_channels=48, kernel_size=2, stride=2), # Block 4 (output)\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=48, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "    )\n",
    "\n",
    "# Now let's build our decoder, with the dimensionality of the latent space matching that of the encoder\n",
    "# and an output size depending on the data.\n",
    "decoder = make_decoder(data_shape, latent_space_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like for the encoder, let's display the structure of the decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "summary(decoder, input_size=(latent_space_size,), batch_dim=0, **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "For the training algorithm, we can copy-paste the generic training code from our fully-connected\n",
    "autoencoders on the MNIST dataset. The only difference is that we want to reconstruct **segmentation maps**\n",
    "(the targets) instead of **grayscale images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define some training hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "def train(forward_pass_fn, encoder, decoder, optimizer, train_data, val_data, device=\"cuda\"):\n",
    "    # Create dataloaders from the data\n",
    "    # Those are PyTorch's abstraction to help iterate over the data\n",
    "    data_loader_kwargs = {\"batch_size\": batch_size, \"num_workers\": os.cpu_count() - 1, \"pin_memory\": True}\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, **data_loader_kwargs)\n",
    "    val_dataloader = DataLoader(val_data, **data_loader_kwargs)\n",
    "\n",
    "    # Ensure that the networks are on the requested device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    fit_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    pbar_metrics = {\"train_loss\": None, \"val_loss\": None}\n",
    "    for epoch in fit_pbar:\n",
    "        # Set model in training mode before training\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Train once over all the training data\n",
    "        for _, y in train_dataloader:\n",
    "            y = y.to(device)    # Move the data tensor to the device\n",
    "            optimizer.zero_grad()   # Make sure gradients are reset\n",
    "            train_loss, _ = forward_pass_fn(encoder, decoder, y)    # Forward pass+loss\n",
    "            train_loss.backward()   # Backward pass\n",
    "            optimizer.step()    # Update parameters w.r.t. optimizer and gradients\n",
    "            pbar_metrics[\"train_loss\"] = train_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        # Set model in eval mode before validation\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # At the end of the epoch, check performance against the validation data\n",
    "        for _, y in val_dataloader:\n",
    "            y = y.to(device)    # Move the data tensor to the device\n",
    "            val_loss, _ = forward_pass_fn(encoder, decoder, y)\n",
    "            pbar_metrics[\"val_loss\"] = val_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass and loss function\n",
    "\n",
    "Unfortunately, we cannot copy-paste the forward pass function from the MNIST notebook.  This is because of the following\n",
    "reasons:\n",
    "\n",
    "1. Here we are predicting segmentation maps with **4 values instead of the 2 black-and-white values** of the MNIST\n",
    "images.  Thus, instead of minimizing the binary cross entropy, we need a **4-class cross-entropy** as the reconstruction\n",
    "term in our **VAE** loss:  \n",
    "$$ CrossEntropy + \\lambda KL_{divergence} $$  \n",
    "as shown in the hands-on document.\n",
    "\n",
    "2. With the MNIST dataset, we were using a fully-connected NN fed with a **vector of pixels** instead of a 2D image.\n",
    "Here, we are using a _convolutional_ autoencoder, so we **need to preserve the 2D structure of our input data**. Since\n",
    "we already receive the images as 2D tensors of pixels, this means we can simply use our inputs as they are, and we don't\n",
    "need to bother with flattening them to vectors anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kl_div(mu, logvar):\n",
    "    kl_div_by_samples = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kl_div_by_samples)\n",
    "\n",
    "def vae_forward_pass(encoder, decoder, x):\n",
    "    \"\"\"VAE forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that predicts a mean and a logvar vector\n",
    "        decoder: neural net that projects a point in the latent space back into the image space\n",
    "        x: batch of N ACDC segmentation maps\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy + kl_divergence loss\n",
    "        x_hat: batch of N reconstructed segmentation maps\n",
    "    \"\"\"\n",
    "    # We don't need to flatten the input images to (N, num_pixels) anymore,\n",
    "    # but we need to convert them from one-channel categorical data to multi-channel one-hot format\n",
    "    encoder_input = torchmetrics.utilities.data.to_onehot(x, num_classes=4).float()\n",
    "\n",
    "    encoding_distr = encoder(encoder_input)  # Forward pass on the encoder (to get the latent space posterior)\n",
    "\n",
    "    # We use the same trick as before to easily extract the components of the posterior distribution (mean and logvar latent vectors)\n",
    "    mu, logvar = encoding_distr[:, :latent_space_size], encoding_distr[:, latent_space_size:]\n",
    "\n",
    "    # Reparametrization trick\n",
    "    # (same as before, since the latent codes are vectors regardless of input data's structure)\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    z = mu + eps * std\n",
    "\n",
    "    # Similar to the input that we didn't need to vectorize, we don't need to reshape the output to a 2D shape anymore,\n",
    "    # since the convolutional network already produces a structured output\n",
    "    x_hat = decoder(z)  # Forward pass on the decoder (to get the reconstructed input)\n",
    "    loss = F.cross_entropy(x_hat, x) # Compute the reconstruction loss\n",
    "    loss += 1e-4 * kl_div(mu, logvar)  # Loss now also includes the KL divergence term\n",
    "    return loss, x_hat.argmax(dim=1) # Transform segmentation back to categorical so that it can be displayed easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready to train the variational autoencoder!\n",
    "Note: this operation may take 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([*encoder.parameters(), *decoder.parameters()])\n",
    "train(vae_forward_pass, encoder, decoder, optimizer, acdc_train, acdc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's take a look at the results on the validation set\n",
    "Note: each time you execute the following cell, you will get different results.\n",
    "\n",
    "If you want better looking reconstructed cardiac shapes, you may retrain your model with more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.visualization.utils import display_autoencoder_results\n",
    "\n",
    "display_autoencoder_results(\n",
    "    acdc_val, lambda x: vae_forward_pass(encoder, decoder, x.cuda())[1], reconstruct_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a dimensionality reduction algorithm, in our case [UMAP](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668),\n",
    "to project the latent space to/from a 2D space we can visualize. Let's try to see an estimation of how the data is\n",
    "distributed in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.visualization.latent_space import explore_latent_space\n",
    "\n",
    "explore_latent_space(\n",
    "    acdc_val,\n",
    "    lambda x: encoder(torchmetrics.utilities.data.to_onehot(x, num_classes=4).float())[:, :latent_space_size],\n",
    "    lambda z: decoder(z).argmax(dim=1),\n",
    "    data_to_encode=\"target\",\n",
    "    batch_size=64,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
